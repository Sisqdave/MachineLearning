---
title: "Practical Machine Learning-Project"
author: "D. Moody"
date: "Sunday, April 26, 2015"
output: html_document
---
#Introduction
This project will create a model on which we can predict the proper form for lifting a barbell by six participants. Data was gathered from accelerometers on the belt, forearm, arm, and barbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The source of this data is http://groupware.les.inf.puc-rio.br/har. On this website. The five different barbell lifts and their associated code are as follows:

exactly according to the specification (Class A),
throwing the elbows to the front (Class B),
lifting the dumbbell only halfway (Class C),
lowering the dumbbell only halfway (Class D) and
throwing the hips to the front (Class E)

The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


#Exploratory analysis and preprocessing
First the datasets and libraries need to be loaded.



pml.training <- read.csv("~/Coursera/MachineLearning/pml-training.csv")

pml.testing <- read.csv("~/Coursera/MachineLearning/pml-testing.csv")
###load libraries
library(caret)
library(plyr)
library(dplyr)

set.seed(20152015)
pmlData <- tbl_df(pml.training)
inTrain <- createDataPartition(pmlData, p=0.70, list=FALSE)
training <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]



Using head(training) I noticed there were a lot of divide by zero entries. I cleaned these out by running the following code. I also removed the X variable as it was an index and seemed to overfit our data. I removed all timestamps. I also remove all data where new window was equal to yes. This data had statistics that seemed to be created by the data authors. I also removed any varaible that contained total or var for the smae reason. At this point I had the measurements from the devices themselves. The nearZeroVar and findCorrelation functions could have been used for this as well.

training <- training[training$new_window=="no",]
dataNA <- lapply(training, mean) == "NA"
NAvar <- names(training[dataNA])
training <- select(training, -one_of(NAvar[2:102]))
var2 <- c("X","raw_timestamp_part_1","raw_timestamp_part_2","num_window" )
training <- select(training, -one_of(var2))
training <- select(training, -contains("total"))
training <- select(training, -contains("var"))


#Model building
I ran the models below on a random sample of the training data to increase speed and get an idea of which model would work best.

trainingSample <- training[sample(nrow(training), 1000), ]
model_rf <- train(classe ~ ., method="rf", data=trainingSample)

model_gbm <- train(classe ~ ., method="gbm", data=trainingSample, verbose=FALSE)

model_rpart <- train(classe ~ ., method="rpart", data=trainingSample)

###Results for using the rf method were as follows:
Overall accuracy was 87.5% with an error rate of 8.8%. this was the best result out of the three methods.

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 8.8%
Confusion matrix:
    A   B   C   D   E class.error
A 281   3   2   1   1  0.02430556
B  16 156  16   2   0  0.17894737
C   1  10 162   3   0  0.07954545
D   4   0  12 148   2  0.10843373
E   1   4   6   4 165  0.08333333

Random Forest 

1000 samples
  49 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
   2    0.8759876  0.8427919  0.02013730   0.02541146
  27    0.8691706  0.8341434  0.01948372   0.02464619
  53    0.8546588  0.8157817  0.02060416   0.02598538

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
###Results for using the rpart method were as follows:
print(modelrpart)
CART 

406 samples
124 predictors
  5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 406, 406, 406, 406, 406, 406, ... 

Resampling results across tuning parameters:

  cp         Accuracy   Kappa      Accuracy SD  Kappa SD 
  0.2323232  0.8105927  0.7626366  0.1663309    0.2073733
  0.2356902  0.7867475  0.7328973  0.1520379    0.1893213
  0.2659933  0.4620719  0.3151455  0.1182348    0.1705846

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was cp = 0.2323232. 
###Results for using the gbm method were as follows:
 model_gbm$results
  shrinkage interaction.depth n.trees  Accuracy     Kappa AccuracySD    KappaSD
1       0.1                 1      50 0.7077024 0.6263477 0.02597220 0.03253433
4       0.1                 2      50 0.7879678 0.7295803 0.02472592 0.03195674
7       0.1                 3      50 0.8201129 0.7708530 0.02132734 0.02768633
2       0.1                 1     100 0.7613844 0.6957977 0.02507748 0.03227129
5       0.1                 2     100 0.8270660 0.7798364 0.01954242 0.02504151
8       0.1                 3     100 0.8500256 0.8091303 0.02366729 0.03015122
3       0.1                 1     150 0.7862485 0.7277430 0.02613613 0.03373108
6       0.1                 2     150 0.8415299 0.7983884 0.01763695 0.02251396
9       0.1                 3     150 0.8586350 0.8201902 0.02058869 0.02622279

confusionMatrix(model_gbm)
Bootstrapped (25 reps) Confusion Matrix 

(entries are percentages of table totals)
 
          Reference
Prediction    A    B    C    D    E
         A 27.7  2.0  0.5  0.5  0.5
         B  0.7 14.6  1.2  0.3  1.1
         C  0.4  1.1 15.4  1.3  0.8
         D  0.2  0.5  1.0 13.2  0.7
         E  0.3  0.6  0.0  0.6 15.0

From the above evidence it is clear that using the rf method has the smallest error rate and the best accuracy.
#Prediction accuracy
accuracy of prediction on test data set
## Highest accuracy was 0.9922265
## Estamated error rate was 0.44%
model_rf_final <- train(classe ~ ., method="rf", data=training)

##results
Random Forest 

19216 samples
   49 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 19216, 19216, 19216, 19216, 19216, 19216, ... 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9922265  0.9901658  0.001131102  0.001432126
  27    0.9928626  0.9909707  0.001341262  0.001699257
  53    0.9854232  0.9815608  0.003682500  0.004652720

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 27

        OOB estimate of  error rate: 0.44%
Confusion matrix:
     A    B    C    D    E class.error
A 5465    5    0    0    1 0.001096692
B   21 3691    6    0    0 0.007261969
C    0    9 3332   11    0 0.005966587
D    0    1   18 3125    3 0.006990785
E    0    1    3    6 3518 0.002834467

confusionMatrix(model_rf)
Bootstrapped (25 reps) Confusion Matrix 

(entries are percentages of table totals)
 
          Reference
Prediction    A    B    C    D    E
         A 28.4  0.1  0.0  0.0  0.0
         B  0.0 19.1  0.1  0.0  0.0
         C  0.0  0.0 17.3  0.2  0.0
         D  0.0  0.0  0.1 16.1  0.0
         E  0.0  0.0  0.0  0.0 18.4

###When run on our test data this is what we got.
results <- predict(model_rf, testing)
table(testing$classe, results)
###Course test data
ctest <- tbl_df(pml.testing)

### Predicting our answers
answers <- predict(model_rf_final, ctest)
answers are B A B A A E D B A A B C B A E E A B B B 
When submitted all of the answers had been predicted correctly.

###Conclusion

Using the train function and the rf method I was able to predict with a 992% accuracy with an error rate of 0.44%.
The model that was tun on the training dataset was 100% accurate on the testing dataset.

Due to time restraints I did not put active code in this document. It is the code that was used. I will create another document and if time permits I will put that one up.